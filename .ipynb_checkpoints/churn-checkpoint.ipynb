{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import pandas.io.sql as psql\n",
    "import psycopg2\n",
    "conn = psycopg2.connect(dbname='churn', \n",
    "                        user='postgres',\n",
    "                        password='postgres',\n",
    "                        host='bi-assess.cvtbaved9qqg.us-west-2.rds.amazonaws.com',\n",
    "                        port=5432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', None, 'display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating the churn attribute\n",
    "\n",
    "Let's start by loading up the datasets into a database, since the intent is to demonstrate competency in SQL. I created a free micro instance on AWS RDS, with 100GB of storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_account = os.path.join('data', 'dim_account', '000')\n",
    "dim_calendar = os.path.join('data', 'dim_calendar', '000')\n",
    "dim_product = os.path.join('data', 'dim_product', '000')\n",
    "dim_territory = os.path.join('data', 'dim_territory', '000')\n",
    "fact_sales_revenue = os.path.join('data', 'fact_sales_revenue', '000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 means open\n",
    "conn.closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dim_account = \"\"\"\n",
    "    CREATE TABLE dim_account (\n",
    "        payer_account_id TEXT, \n",
    "        payer_account_company_name TEXT,\n",
    "        payer_account_registration_date TIMESTAMP WITHOUT TIME ZONE,\n",
    "        payer_account_status_code TEXT,\n",
    "        payer_account_role_name TEXT,\n",
    "        payer_account_creation_date TIMESTAMP WITHOUT TIME ZONE,\n",
    "        payer_account_last_update_date TIMESTAMP WITHOUT TIME ZONE,\n",
    "        payer_account_registration_tax_country_code TEXT,\n",
    "        is_payer_account_bill_in_90_days CHAR(1),\n",
    "        is_payer_account_enterprise CHAR(1),\n",
    "        is_payer_account_domain_free_domain CHAR(1),\n",
    "        is_payer_account_on_enterprise_support CHAR(1),\n",
    "        is_payer_account_on_business_support CHAR(1),\n",
    "        is_payer_account_fraud CHAR(1),\n",
    "        is_payer_account_suspended CHAR(1),\n",
    "        is_payer_account_internal CHAR(1),\n",
    "        is_payer_account_inp_terminated CHAR(1),\n",
    "        is_payer_account_on_behalf_of CHAR(1),\n",
    "        is_payer_account_tax_exempt CHAR(1),\n",
    "        is_payer_account_reseller CHAR(1),\n",
    "        payer_account_first_billing_reporting_date TIMESTAMP WITHOUT TIME ZONE,\n",
    "        payer_account_first_payment_reporting_date TIMESTAMP WITHOUT TIME ZONE,\n",
    "        payer_account_first_usage_date TIMESTAMP WITHOUT TIME ZONE\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "create_dim_calendar = \"\"\"\n",
    "    CREATE TABLE dim_calendar (\n",
    "        calendar_sid INTEGER,\n",
    "        calendar_date DATE,\n",
    "        day_of_week_name TEXT,\n",
    "        day_of_week_number INTEGER,\n",
    "        day_of_week_code TEXT,\n",
    "        day_of_quarter_number INTEGER,\n",
    "        day_of_year_number INTEGER,\n",
    "        calendar_period TEXT,\n",
    "        week_id INTEGER,\n",
    "        prior_week_id INTEGER,\n",
    "        subsequent_week_id INTEGER,\n",
    "        week_number INTEGER,\n",
    "        week_code TEXT,\n",
    "        month_id INTEGER,\n",
    "        prior_month_id INTEGER,\n",
    "        subsequent_month_id INTEGER,\n",
    "        month_number INTEGER,\n",
    "        month_code TEXT,\n",
    "        month_num_of_days INTEGER,\n",
    "        year_id INTEGER,\n",
    "        prior_year_id INTEGER,\n",
    "        subsequent_year_id INTEGER,\n",
    "        year_number INTEGER,\n",
    "        year_code TEXT,\n",
    "        ytd_flag BOOLEAN,\n",
    "        yoy_flag BOOLEAN,\n",
    "        report_month_flag BOOLEAN,\n",
    "        prospecting_day_flag BOOLEAN\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "create_dim_product = \"\"\"\n",
    "    CREATE TABLE dim_product (\n",
    "        product_sid TEXT,\n",
    "        product_line_name TEXT,\n",
    "        product_name TEXT,\n",
    "        sub_product_name TEXT,\n",
    "        service_group_name TEXT,\n",
    "        product_companyn_name TEXT\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "create_dim_territory = \"\"\"\n",
    "    CREATE TABLE dim_territory (\n",
    "        territory_sid TEXT,\n",
    "        region TEXT,\n",
    "        sub_region TEXT,\n",
    "        territory_code TEXT,\n",
    "        segment TEXT\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "create_fact_sales_revenue = \"\"\"\n",
    "    CREATE TABLE fact_sales_revenue (\n",
    "       territory_sid TEXT,\n",
    "       payer_account_id TEXT,\n",
    "       month_id INTEGER,\n",
    "       product_sid TEXT,\n",
    "       sales_revenue NUMERIC(38,6)\n",
    "    );\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\n",
    "#    'dim_account',\n",
    "#    'dim_calendar',\n",
    "#    'dim_product',\n",
    "#    'dim_territory',\n",
    "#    'fact_sales_revenue'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_print(query):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(query)\n",
    "        data = cur.fetchall()\n",
    "        print(data)\n",
    "        conn.commit()\n",
    "    except psycopg2.OperationalError as oe:\n",
    "        print(oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(tablename):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        func_name = 'create_' + tablename\n",
    "        cur.execute(eval(func_name))\n",
    "        conn.commit()\n",
    "        print('Table ' + tablename + ' created.')\n",
    "    except psycopg2.OperationalError as oe:\n",
    "        print(oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_from_file(filepath, tablename):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        f = open(filepath, 'r')\n",
    "        cur.copy_from(f, tablename, sep=',')\n",
    "        conn.commit()\n",
    "        f.close()\n",
    "        print(filepath + ' copied into table ' + tablename + '.')\n",
    "    except psycopg2.OperationalError as oe:\n",
    "        print(oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_only(query):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        print('Done.')\n",
    "    except psycopg2.OperationalError as oe:\n",
    "        print(oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for table in tables:\n",
    "    create_table(table)\n",
    "    copy_from_file(eval(table), table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First there are two things I want to be sure of: \n",
    "\n",
    "1. That each row in dim_account is distinct, i.e. we are storing state instead of transactions. E.g. when an account is suspended, and UPDATE is done, not an INSERT\n",
    "\n",
    "2. That each row in fact_revenue_sales is the total monthly sales for that month_id, i.e. we are storing aggregates instead of transactions. If not, we need to aggregate the transactions to get the total monthly sales, since churn is a function of change in monthly sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "select_print('SELECT COUNT(*) from dim_account;')\n",
    "select_print('SELECT COUNT(DISTINCT payer_account_id) FROM dim_account;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both selects have the same value of 21053889, we've confirmed that dim_account stores the state of each account as identified by payer_account_id.\n",
    "\n",
    "We have a lot of payer accounts, and the file dump of the fact_sales_revenue was roughly 1/4 the size of dim_account. How many accounts even have any transactions at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "select_print('SELECT COUNT(DISTINCT payer_account_id) FROM fact_sales_revenue;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mere 0.69% of payer accounts actually have transactions on file.. Let's set dim_account aside for now, and dive into fact_revenue_sales to answer the second question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_num_transactions_per_month = \"\"\"\n",
    "    SELECT num_transactions, COUNT(payer_account_id) AS num_account_months\n",
    "    FROM (SELECT payer_account_id, month_id, COUNT(sales_revenue) as num_transactions\n",
    "        FROM fact_sales_revenue\n",
    "        GROUP BY payer_account_id, month_id) subquery\n",
    "    GROUP BY num_transactions\n",
    "    ORDER BY num_transactions ASC;\n",
    "\"\"\"\n",
    "\n",
    "num_transactions_per_month_df = psql.read_sql(select_num_transactions_per_month, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "X = num_transactions_per_month_df.num_transactions\n",
    "Y = num_transactions_per_month_df.num_account_months\n",
    "\n",
    "ax = sns.barplot(x=X, y=Y)\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "ax.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a distribution of the number of transactions an account makes in a single month. It is of course heavily skewed toward a low number of transactions within a single month, but we see an interested uptick at 5 transactions per month, from whence it declines again. \n",
    "\n",
    "This suggests that there are at least two distributions of low-transaction payer-months, one which declines sharply from 1 transaction, and another which declines more gradually from 6 transactions per month.\n",
    "\n",
    "Business-wise this means that there is some differentiable group of player-months where the number of transactions is consistantly high. This could mean either an opportunity to optimize by consolidating payments, or an opportunity to find out what is leading to more transactions in the latter group, and use it to increase the transaction count in the former group. \n",
    "\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we have not answered the second question we initially had: whether or not there is a single transaction per payer-month, i.e. whether the value in sales_revenue is for the entire month, or if we need to aggregate them per payer-month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "select_multiple_transactions_per_payer_month = \"\"\"\n",
    "    SELECT * \n",
    "    FROM (SELECT payer_account_id, month_id, COUNT(sales_revenue) AS num_transactions\n",
    "        FROM fact_sales_revenue\n",
    "        GROUP BY payer_account_id, month_id) subquery\n",
    "    WHERE num_transactions > 1\n",
    "    LIMIT 100;\n",
    "\"\"\"\n",
    "\n",
    "multiple_transactions_per_payer_month_df = psql.read_sql(select_multiple_transactions_per_payer_month, conn)\n",
    "print(multiple_transactions_per_payer_month_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And right away we can see easily enough that there are plenty of payers with more than one sales_revenue per month. Our hunch was correct, this is transactional data that needs to be aggregated.\n",
    "\n",
    "Let's create another table to store the total monthly sales revenue of each payer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# select_total_revenue_per_payer_month = \"\"\"\n",
    "#     SELECT payer_account_id, month_id, SUM(sales_revenue) as month_sales_revenue\n",
    "#     INTO TABLE monthly_sales_revenue\n",
    "#     FROM fact_sales_revenue\n",
    "#     GROUP BY payer_account_id, month_id\n",
    "# \"\"\"\n",
    "\n",
    "# execute_only(select_total_revenue_per_payer_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's create a monthly calendar table for joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_monthly_calendar = \"\"\"\n",
    "#     SELECT month_id, month_number, year_number\n",
    "#     INTO monthly_calendar\n",
    "#         FROM dim_calendar\n",
    "#         GROUP BY month_id, month_number, year_number\n",
    "# \"\"\"\n",
    "\n",
    "# execute_only(create_monthly_calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to add a column storing the churn attribute. \n",
    "\n",
    "An account is defined as having churned in a given month if its total transaction revenue in the previous year's month of December was above 1, but the same value in the month in concern is below 1. We can phrase this as a boolean conditional:\n",
    "\n",
    "```\n",
    "churn = previous_dec_revenue > 1 and month_sales_revenue < 1\n",
    "```\n",
    "\n",
    "We already created the month_sales_revenue attribute, so we just need to create the previous_dec_revenue attribute, then create the churn attribute based on these two attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# select_into_churn = \"\"\"\n",
    "#     SELECT msr.payer_account_id, \n",
    "#         msr.month_id, mc.month_number, \n",
    "#         mc.year_number, \n",
    "#         msr.month_sales_revenue, \n",
    "#         mc2.year_number AS previous_year_number, \n",
    "#         mc2.month_id AS previous_dec_month_id, \n",
    "#         msr2.month_sales_revenue AS previous_dec_month_sales_revenue, \n",
    "#         (msr2.month_sales_revenue > 1 AND msr.month_sales_revenue < 1) AS churn\n",
    "#     INTO churn\n",
    "#         FROM monthly_sales_revenue msr\n",
    "#         LEFT JOIN monthly_calendar mc ON msr.month_id = mc.month_id\n",
    "#         LEFT JOIN monthly_calendar mc2 ON mc.year_number-1 = mc2.year_number AND mc2.month_number = 12\n",
    "#         LEFT JOIN monthly_sales_revenue msr2 ON mc2.month_id = msr2.month_id AND msr2. payer_account_id = msr.payer_account_id\n",
    "# \"\"\"\n",
    "\n",
    "# execute_only(select_into_churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something I learnt about Postgres' handling of boolean values:\n",
    "\n",
    "```\n",
    "null AND false = false\n",
    "null AND true = null\n",
    "```\n",
    "\n",
    "and the same goes if you swap the places of the conditions. \n",
    "\n",
    "This means that in our resulting table, the meanings of the churn variable are:\n",
    "-  true : previous_dec_revenue > 1 __and__ month_sales_revenue < 1 \n",
    "-  null : previous_dec_revenue = null __and__ month_sales_revenue < 1\n",
    "-  false: previous_dec_revenue < 1 __or__ month_sales_revenue > 1, since null previous_dec_revenue means it was 0\n",
    "\n",
    "In other words, we can consider null churn to be the same as false, since it means that previous_dec_revenue == 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we have created the churn variable in the database. The next step is to find out the factors influencing churn, and since it's much more exploratory, I'll try to do most of it in Python using Pandas. There are 1160280 rows in the churn table, so it should be managable. Of course, I'll still have to make SQL queries to retrieve data to engineer features with. \n",
    "\n",
    "For more notes on the problems I faced along the way creating the churn variable, and how I approached them, please see the footnote at the bottom of this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Identifying the causes of churn\n",
    "\n",
    "Churn is an attribute identified by two factors: payer_account_id, and month_id. Hence, whatever is affecting churn must be a characteristic of one of these two entities: payer accounts, and the month in concern.\n",
    "\n",
    "Since our time aggregation is on a monthly level, we don't really have things like day of week to worry about; it's unlikely that the number of days in a month is a factor related to churn. We do have to contend with how to treat the ordinal nature of transactions when predicting churn, but let's worry about that later on. \n",
    "\n",
    "With regard to payer accounts, we can break descriptors down into two components: account attributes, and transaction attributes. Transaction attributes in aggregate also act as account attributes. \n",
    "\n",
    "Account attributes are available simply by joining dim_account onto churn. We already know that dim_account comprises distinct rows for each account, so no preprocessing needed there. \n",
    "\n",
    "Transaction attributes are available from dim_product and dim_territory. Before we join these tables onto churn, we need to better understand them the same way we figured out dim_account. First, let's confirm the level of aggregation of these two tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "select_print('SELECT COUNT(*) from dim_product;')\n",
    "select_print('SELECT COUNT(DISTINCT product_sid) FROM dim_product;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "select_print('SELECT COUNT(*) from dim_territory;')\n",
    "select_print('SELECT COUNT(DISTINCT territory_sid) FROM dim_territory;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've confirmed that these are mapping tables containing attributes of products and territories. However, products and territories are aggregated in fact_sales_revenue on a transaction basis, while churn is labeled on an aggregated monthly basis. \n",
    "\n",
    "In order to understand churn on a finer grain, transactional level, let's map churn back to fact_sale_revenue, and add all the product and territory attributes in for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# select_into_churn_factors = \"\"\"\n",
    "#     SELECT fsr.territory_sid, fsr.payer_account_id, fsr.month_id, fsr.product_sid, fsr.sales_revenue, \n",
    "#         churn.month_number, churn.year_number, churn.month_sales_revenue, churn.previous_dec_month_sales_revenue, churn.churn,\n",
    "#         dt.region, dt.sub_region, dt.territory_code, dt.segment,\n",
    "#         dp.product_line_name, dp.product_name, dp.sub_product_name, dp.service_group_name, dp.product_companyn_name\n",
    "#     INTO churn_factors\n",
    "#         FROM fact_sales_revenue fsr\n",
    "#         LEFT JOIN churn ON fsr.payer_account_id = churn.payer_account_id AND fsr.month_id = churn.month_id\n",
    "#         LEFT JOIN dim_territory dt ON dt.territory_sid = fsr.territory_sid\n",
    "#         LEFT JOIN dim_product dp ON dp.product_sid = fsr.product_sid\n",
    "#         ;\n",
    "# \"\"\"\n",
    "\n",
    "# execute_only(select_into_churn_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's done, let's begin trying to work with the data locally to figure out what the churn risk factors are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_factors_filepath = os.path.join('output', 'churn_factors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "# select_churn_factors = \"\"\"\n",
    "# SELECT *\n",
    "#     FROM churn_factors\n",
    "#     ORDER BY payer_account_id, month_id, territory_sid, product_sid ASC;\n",
    "# \"\"\"\n",
    "\n",
    "# df = psql.read_sql(select_churn_factors, conn)\n",
    "# df.to_csv(churn_factors_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(churn_factors_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do need to re-aggregate this table back into month_sales_revenue, and create new factors that are aggregates of the transaction-wise features, but for now let's take a look at the transactions themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_plot(df):\n",
    "    fig, axes = plt.subplots(nrows=5, ncols=2)\n",
    "    fig.set_size_inches(16, 20)\n",
    "\n",
    "    df.month_number.value_counts(dropna=False).sort_index().plot('bar', title='Transactions per month', ax=axes[0,0])\n",
    "    df.year_number.value_counts(dropna=False).sort_index().plot('bar', title='Transactions per year', ax=axes[0,1])\n",
    "    df.month_id.value_counts(dropna=False).sort_index().plot('bar', title='Transactions per month in year', ax=axes[1,0])\n",
    "    df.region.value_counts(dropna=False).plot('barh', title='Transactions per region', ax=axes[1,1])\n",
    "    df.sub_region.value_counts(dropna=False).plot('barh', title='Transactions per sub-region', ax=axes[2,0])\n",
    "    df.territory_code.value_counts(dropna=False).plot('barh', title='Transactions per territory', ax=axes[2,1])\n",
    "    df.product_line_name.value_counts(dropna=False).plot('bar', title='Transactions per product line', ax=axes[3,0])\n",
    "    df.sub_product_name.value_counts(dropna=False).plot('bar', title='Transactions per sub_product', ax=axes[3,1])\n",
    "    df.service_group_name.value_counts(dropna=False).plot('bar', title='Transactions per sub-product', ax=axes[4,0])\n",
    "    df.churn.value_counts(dropna=False).plot('barh', title='Transactions by churned and non-churned payers', ax=axes[4,1])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explore_plot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For whatever reason, these two features don't plot well,\n",
    "# probably because the skew is rather extreme and there are many possible values.\n",
    "print(df.product_name.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.product_companyn_name.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's eyeball the same charts for only the transactions of customers that churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_transactions_df = df[df['churn'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explore_plot(churn_transactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance, the only stark difference is that there are more transactions by churned payer accounts in January and February."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to understand feature importance, and importance is really relative to some purpose. In our case, it should be a feature's importance to determining whether a customer will churn; so, any feature importance we engineer will be relative to the method used to determine whether a customer will churn. However, we might interpret the general overall ranking of features to be related to the use of other classification methods.\n",
    "\n",
    "For this example, we will use catboost's CatBoostClassifier to try to determine what features affect churn. We will engineer some month-aggregated features, then try to fit the classifier to see how much the features contribute to predicting it. These are the features we will engineer: \n",
    "\n",
    "- cumulative_num_transactions\n",
    "- num_transactions\n",
    "- mean_num_transactions_per_month\n",
    "- max_num_transactions_per_month\n",
    "- min_num_transactions_per_month\n",
    "- cumulative_revenue\n",
    "- total_revenue_in_month\n",
    "- mean_transaction_revenue_per_month\n",
    "- max_transaction_revenue_per_month\n",
    "- min_transaction_revenue_per_month\n",
    "- top_product_line_name\n",
    "- top_product_line_num_transactions\n",
    "- top_product_line_percent_transactions\n",
    "- top_product_line_revenue\n",
    "- top_product_line_revenue_percent\n",
    "- top_product_name\n",
    "- top_product_num_transactions\n",
    "- top_product_percent_transactions\n",
    "- top_product_cumulative_revenue\n",
    "- top_product_cumulative_revenue_percent\n",
    "- num_product_lines\n",
    "- num_product_names\n",
    "- num_sub_product_names\n",
    "\n",
    "There are many other features we could engineer, but the more there are the longer it takes to train the model, and I feel like I'm overthinking this assignment already, so let's begin with just this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['payer_account_id', 'month_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feature_df = grouped['sales_revenue'].agg('count').groupby(level=[0]).cumsum().to_frame()\n",
    "feature_df.columns=['cumulative_num_transactions']\n",
    "feature_df['num_transactions'] = grouped['sales_revenue'].agg('count')\n",
    "mean_transactions = feature_df.reset_index().groupby(['payer_account_id']).expanding().agg('mean')['num_transactions']\n",
    "mean_transactions.index = feature_df.index\n",
    "#timing up to here\n",
    "feature_df['mean_num_transactions_per_month'] = mean_transactions\n",
    "max_transactions = feature_df.reset_index().groupby(['payer_account_id']).expanding().agg('max')['num_transactions']\n",
    "max_transactions.index = feature_df.index\n",
    "feature_df['max_num_transactions_per_month'] = max_transactions\n",
    "min_transactions = feature_df.reset_index().groupby(['payer_account_id']).expanding().agg('min')['num_transactions']\n",
    "min_transactions.index = feature_df.index\n",
    "feature_df['min_num_transactions_per_month'] = min_transactions\n",
    "\n",
    "cum_rev = grouped['sales_revenue'].agg(np.sum).groupby(level=[0]).cumsum()\n",
    "feature_df['cumulative_revenue'] = cum_rev\n",
    "feature_df['total_revenue_in_month'] = grouped['sales_revenue'].agg('sum')\n",
    "mean_revenue = feature_df.reset_index().groupby(['payer_account_id']).expanding().agg('mean')['total_revenue_in_month']\n",
    "mean_revenue.index = feature_df.index\n",
    "feature_df['mean_transaction_revenue_per_month'] = mean_revenue\n",
    "max_revenue = feature_df.reset_index().groupby(['payer_account_id']).expanding().agg('max')['total_revenue_in_month']\n",
    "max_revenue.index = feature_df.index\n",
    "feature_df['max_transaction_revenue_per_month'] = max_revenue\n",
    "min_revenue = feature_df.reset_index().groupby(['payer_account_id']).expanding().agg('min')['total_revenue_in_month']\n",
    "min_revenue.index = feature_df.index\n",
    "feature_df['min_transaction_revenue_per_month'] = min_revenue\n",
    "\n",
    "feature_df['top_product_line_name'] = grouped['product_line_name'].agg(pd.Series.mode)\n",
    "feature_df['product_line_name_list'] = grouped['product_line_name'].apply(list)\n",
    "feature_df['top_product_line_num_transactions'] = feature_df.apply(lambda x: x['product_line_name_list'].count(x['top_product_line_name']) \n",
    "                                                             if type(x['top_product_line_name']) == str\n",
    "                                                            else x['product_line_name_list'].count(x['top_product_line_name'][0]), axis=1)\n",
    "feature_df['top_product_line_percent_transactions'] = feature_df.apply(lambda x: x['top_product_line_num_transactions']/x['num_transactions'], axis=1)\n",
    "\n",
    "feature_df['num_product_lines'] = grouped['product_line_name'].nunique()\n",
    "feature_df['num_product_names'] = grouped['product_name'].nunique()\n",
    "feature_df['num_sub_product_names'] = grouped['sub_product_name'].nunique()\n",
    "\n",
    "feature_df.to_csv(os.path.join('output', 'feature.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1160280, 19)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df = pd.read_csv(os.path.join('output', 'feature.csv'))\n",
    "feature_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll just pull out the churn and date features and join them so we can save the processing time of aggregating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_filepath = os.path.join('output', 'churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "# select_churn = \"\"\"\n",
    "# SELECT *\n",
    "#     FROM churn\n",
    "#     ORDER BY payer_account_id, month_id;\n",
    "# \"\"\"\n",
    "\n",
    "# df = psql.read_sql(select_churn, conn)\n",
    "# df.to_csv(churn_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = pd.read_csv(churn_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = churn_df[['payer_account_id', 'month_id', 'month_number', 'year_number', 'churn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1160280, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = feature_df.merge(churn_df, on=['payer_account_id', 'month_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1160280, 22)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('churn', axis=1)\n",
    "y = df.churn.map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.dtypes)\n",
    "\n",
    "categorical_features_indices = np.where(X.dtypes != np.float)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 5432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    iterations=5000,\n",
    "    learning_rate=0.03,\n",
    "    random_seed=5432,\n",
    "    thread_count = 8,\n",
    "    use_best_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    cat_features=categorical_features_indices,\n",
    "    eval_set=(X_test, y_test),\n",
    "#     logging_level='Verbose',  # you can uncomment this for text output\n",
    "    plot=True\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost.save_model(model, os.path.join('output', 'trained_catboostclassifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_score = pd.DataFrame(list(zip(X.dtypes.index, model.get_feature_importance(Pool(X, label=y, cat_features=categorical_features_indices)))),\n",
    "                columns=['Feature','Score'])\n",
    "\n",
    "feature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,7)\n",
    "ax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\n",
    "ax.set_title(\"Catboost Feature Importance Ranking\", fontsize = 14)\n",
    "ax.set_xlabel('')\n",
    "\n",
    "rects = ax.patches\n",
    "\n",
    "labels = feature_score['Score'].round(2)\n",
    "\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 0.35, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnote\n",
    "Along the way I ran into problems running queries that simply took too long, and I had to figure out what I was doing wrong... \n",
    "\n",
    "I starting with UPDATE to create calculated attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update monthly_sales_revenue with a column containing previous_dec_revenue\n",
    "\n",
    "# create_prev_dec_revenue = \"\"\"\n",
    "#     ALTER TABLE monthly_sales_revenue\n",
    "#     ADD COLUMN previous_dec_month_sales_revenue NUMERIC(38,6);\n",
    "# \"\"\"\n",
    "\n",
    "# set_prev_dec_revenue = \"\"\"\n",
    "#     UPDATE monthly_sales_revenue \n",
    "#     SET previous_dec_month_sales_revenue = subquery.previous_dec_month_sales_revenue\n",
    "#     FROM (SELECT msr.payer_account_id, msr.month_id, dc.month_number, dc.year_number, msr.month_sales_revenue, dc2.year_number AS previous_year_number, dc2.month_id AS previous_dec_month_id, msr2.month_sales_revenue AS previous_dec_month_sales_revenue\n",
    "#         FROM monthly_sales_revenue msr\n",
    "#         LEFT JOIN dim_calendar dc ON msr.month_id = dc.month_id\n",
    "#         LEFT JOIN dim_calendar dc2 ON dc.year_number-1 = dc2.year_number AND dc2.month_number = 12\n",
    "#         LEFT JOIN monthly_sales_revenue msr2 ON dc2.month_id = msr2.month_id) subquery\n",
    "#     WHERE monthly_sales_revenue.payer_account_id = subquery.payer_account_id \n",
    "#     AND monthly_sales_revenue.month_id = subquery.month_id\n",
    "# \"\"\"\n",
    "\n",
    "# execute_only(create_prev_dec_revenue)\n",
    "# execute_only(set_prev_dec_revenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and when it took too long, I figured selecting into another table would be faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new table containing the previous_dec_revenue attribute, which should be faster than UPDATE\n",
    "\n",
    "# %%time\n",
    "# select_revenue_comparison = \"\"\"\n",
    "#     SELECT msr.payer_account_id, msr.month_id, dc.month_number, dc.year_number, msr.month_sales_revenue, dc2.year_number AS previous_year_number, dc2.month_id AS previous_dec_month_id, msr2.month_sales_revenue AS previous_dec_month_sales_revenue\n",
    "#     INTO TABLE revenue_comparison\n",
    "#     FROM monthly_sales_revenue msr\n",
    "#         LEFT JOIN dim_calendar dc ON msr.month_id = dc.month_id\n",
    "#         LEFT JOIN dim_calendar dc2 ON dc.year_number-1 = dc2.year_number AND dc2.month_number = 12\n",
    "#         LEFT JOIN monthly_sales_revenue msr2 ON dc2.month_id = msr2.month_id\n",
    "# \"\"\"\n",
    "\n",
    "# execute_only(select_revenue_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried using a function to store variables so that hopefully creating them procedurally would avoid consuming memory on the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a function to generate previous_dec_revenue procedurally\n",
    "\n",
    "# function_previous_dec_revenue = \"\"\"\n",
    "#     CREATE FUNCTION previous_dec_monthly_sales_revenue(monthly_sales_revenue)\n",
    "#     RETURNS NUMERIC(38,6) AS\n",
    "#     $func$\n",
    "#     SELECT previous_dec_month_sales_revenue \n",
    "#     FROM (SELECT msr.payer_account_id, msr.month_id, dc.month_number, dc.year_number, msr.month_sales_revenue, dc2.year_number AS previous_year_number, dc2.month_id AS previous_dec_month_id, msr2.month_sales_revenue AS previous_dec_month_sales_revenue\n",
    "#         FROM monthly_sales_revenue msr\n",
    "#         LEFT JOIN dim_calendar dc ON msr.month_id = dc.month_id\n",
    "#         LEFT JOIN dim_calendar dc2 ON dc.year_number-1 = dc2.year_number AND dc2.month_number = 12\n",
    "#         LEFT JOIN monthly_sales_revenue msr2 ON dc2.month_id = msr2.month_id) subquery\n",
    "#     $func$ LANGUAGE SQL STABLE; \n",
    "# \"\"\"\n",
    "\n",
    "# execute_only(function_previous_dec_revenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the above approaches worked out in the sense of being able to be executed in a reasonable amount of time. In the case of the stored function, a simple select query with a very low limit faced the same problem.\n",
    "\n",
    "So, I tried to break it down into smaller steps even if it winds up looking less elegant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is make sure that I'm not crazy, and that my SQL works. Since instances where there was no transaction will come up as NULL, I'll omit those and limit the result to 500 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# test_revenue_comparison = \"\"\"\n",
    "# SELECT msr.payer_account_id, msr.month_id, dc.month_number, dc.year_number, msr.month_sales_revenue, dc2.year_number AS previous_year_number, dc2.month_id AS previous_dec_month_id, msr2.month_sales_revenue AS previous_dec_month_sales_revenue\n",
    "#     FROM monthly_sales_revenue msr\n",
    "#     LEFT JOIN dim_calendar dc ON msr.month_id = dc.month_id\n",
    "#     LEFT JOIN dim_calendar dc2 ON dc.year_number-1 = dc2.year_number AND dc2.month_number = 12\n",
    "#     LEFT JOIN monthly_sales_revenue msr2 ON dc2.month_id = msr2.month_id\n",
    "# WHERE msr2.month_sales_revenue IS NOT NULL\n",
    "# LIMIT 500;\n",
    "# \"\"\"\n",
    "\n",
    "# test_revenue_comparison_df = psql.read_sql(test_revenue_comparison, conn)\n",
    "# print(test_revenue_comparison_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test_revenue_baseline = \"\"\"\n",
    "#     SELECT payer_account_id, month_id, month_sales_revenue \n",
    "#     FROM monthly_sales_revenue\n",
    "#     WHERE payer_account_id LIKE 'PA000000000005593129'\n",
    "#     AND month_id = 201512;\n",
    "# \"\"\"\n",
    "\n",
    "# select_print(test_revenue_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No reason that wouldn't have been right, but it couldn't be any clearer after seeing the result of these two queries match up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I thought that I would have to index the tables, which I didn't want to do because I figured it wouldn't be worth it since I would only be querying them once to create the churn table which I would be using for my analysis.\n",
    "\n",
    "At this point, I realized my mistake: The dim_calendar table is aggregated on a daily basis! Every join on month_id was duplicating the entire table ~30 times, and since I was joining twice that was 900 times. \n",
    "\n",
    "Using EXPLAIN also led me to realize that I also made an error in the LEFT JOIN of msr2; I have to join it on payer_account_id as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to clean up commits if an error is thrown.\n",
    "\n",
    "curs = conn.cursor()\n",
    "curs.execute(\"ROLLBACK\")\n",
    "conn.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
